{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习纳米学位\n",
    "## 猫狗大战 Dog vs Cat\n",
    "### 一、 分析数据，准备数据\n",
    "首先从[kaggle](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data)上下载好数据包:test.zip和train.zip，对包进行解压缩与数据预处理。\n",
    "#### 1、解压缩文件\n",
    "注意将zip文件放到该ipythonnotebook同级别目录下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "def un_zip(file_name):  \n",
    "    \"\"\"unzip zip file\"\"\"\n",
    "    if not os.path.isdir(file_name):\n",
    "        zip_file = zipfile.ZipFile(file_name+\".zip\")  \n",
    "        zip_file.extractall()\n",
    "        zip_file.close()\n",
    "train_dir = 'train'\n",
    "test_dir = 'test'\n",
    "un_zip(test_dir)\n",
    "un_zip(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察解压出来的数据，可以看到数据名字格式，例如训练集中cat.0.jpg表示猫分类的第一张图，同理dog.0.jpg表示狗分类的第一张图，数字从0一直持续到12499，猫和狗各12500张，测试集中类似，只是没有猫狗的标记，图像只是以数字计算，一共有12500张测试图片。\n",
    "#### 2、 建立分类目录\n",
    "\n",
    "我们为数据文件建立symbol link并划分为训练集,测试集，为模型训练和特征提取打好基础。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "work_dir  = os.getcwd()\n",
    "train_dir = work_dir + '/train/'\n",
    "test_dir = work_dir + '/test/'\n",
    "# 建立link根目录\n",
    "imglink_dir = work_dir + \"/img_link/\"\n",
    "if(os.path.exists(imglink_dir)):\n",
    "    shutil.rmtree(imglink_dir)\n",
    "os.mkdir(imglink_dir)\n",
    "# 建立特征提取训练集目录\n",
    "img_train2 = imglink_dir + \"train2/\"\n",
    "os.mkdir(img_train2)\n",
    "os.mkdir(img_train2 + \"cat/\")\n",
    "os.mkdir(img_train2 + \"dog/\")\n",
    "# 建立测试集目录\n",
    "img_test  = imglink_dir + \"test/\"\n",
    "os.mkdir(img_test)\n",
    "os.mkdir(imglink_dir + \"test/mixed/\")\n",
    "# 建立模型训练训练集 验证集目录，测试集目录共用一个\n",
    "img_train = imglink_dir + \"train/\"\n",
    "os.mkdir(img_train)\n",
    "img_valid = imglink_dir + \"valid/\"\n",
    "os.mkdir(img_valid)\n",
    "img_train_cat = img_train + \"cat/\"\n",
    "os.mkdir(img_train_cat)\n",
    "img_train_dog = img_train + \"dog/\"\n",
    "os.mkdir(img_train_dog)\n",
    "img_valid_cat = img_valid + \"cat/\"\n",
    "os.mkdir(img_valid_cat)\n",
    "img_valid_dog = img_valid + \"dog/\"\n",
    "os.mkdir(img_valid_dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#link test\n",
    "test_filenames = os.listdir(test_dir)\n",
    "num_test = len(test_filenames)\n",
    "\n",
    "for i in tqdm(range(num_test)): \n",
    "    os.symlink(test_dir + test_filenames[i], img_test + \"mixed/\" + test_filenames[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3、 数据研究\n",
    "先来展示下咱们的训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, file in enumerate(os.listdir(train_dir)[:8]):\n",
    "    img = plt.imread(os.path.join(train_dir, file))\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plt.title(file.split('.')[0])\n",
    "    plt.axis('on')\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的图示的结果来看，可以看出每张图片并不像我们预想的分辨率都一样大，那么，让获取每张图片的分辨率以及通道数，来对分辨率进行统计分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_info = [[],[]]\n",
    "outside = []\n",
    "train_img_list = list(filter(lambda x:x[-3:] == 'jpg', os.listdir(train_dir)))\n",
    "for img in tqdm(train_img_list):\n",
    "    info = cv2.imread(os.path.join(train_dir, img))\n",
    "    train_info[0].append(info.shape[0])\n",
    "    train_info[1].append(info.shape[1])\n",
    "    if(info.shape[0] > 600):\n",
    "        outside.append(train_dir+img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(outside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图示化分析图片的分辨率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(train_info[0], train_info[1])\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height') \n",
    "plt.xlim(0,max(train_info[0]) + 10)\n",
    "plt.ylim(0,max(train_info[1]) + 10)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从图表中可以看出图片的宽度和高度基本都遍布在100-500之间。\n",
    "\n",
    "删除那两张超出平均分辨率很多的图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for img in outside:\n",
    "    os.remove(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4、数据预处理\n",
    "为了尽量利用我们有限的训练数据，我们将通过一系列随机变换堆数据进行提升，这样我们的模型将看不到任何两张完全相同的图片，这有利于我们抑制过拟合，使得模型的泛化能力更好。\n",
    "\n",
    "这个步骤可以通过keras.preprocessing.image.ImageGenerator来实现，下面我们来展示下这个预处理函数实现后的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "temp = ImageDataGenerator(\n",
    "                        rotation_range = 10,\n",
    "                        zoom_range = 0.2,\n",
    "                        width_shift_range = 0.05,\n",
    "                        height_shift_range = 0.1,\n",
    "                        channel_shift_range=10,\n",
    "                        horizontal_flip=True)\n",
    "\n",
    "img = load_img(train_dir + '/cat.0.jpg')  \n",
    "x = img_to_array(img)  \n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "if(os.path.exists(\"preview\")):\n",
    "    shutil.rmtree(\"preview\")\n",
    "os.mkdir(\"preview\")\n",
    "i = 0\n",
    "for batch in temp.flow(x, batch_size=1,\n",
    "                          save_to_dir='preview', save_prefix='cat', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i >= 8:\n",
    "        break\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, file in enumerate(os.listdir(\"preview\")):\n",
    "    img = plt.imread(os.path.join(\"preview\", file))\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plt.axis('on')\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着要进行我们的数据清洗工作，因为我们的图片保不齐有一些图并不是我们的猫狗类别的图像，这样对我们的算法上可能会造成一些影响，所以要对不是猫狗的图进行一下清洗。清洗的方法参考[这个链接](https://zhuanlan.zhihu.com/p/34068451)，使用imagenet上top几来进行分类。我还找到了一个[马桶的例子](https://blog.csdn.net/lauyeed/article/details/78886830)，感觉也很实用。\n",
    "\n",
    "首先把前辈整理好的猫狗种类偷下来整理成列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dogs_cats = [\n",
    " 'n02085620','n02085782','n02085936','n02086079'\n",
    ",'n02086240','n02086646','n02086910','n02087046'\n",
    ",'n02087394','n02088094','n02088238','n02088364'\n",
    ",'n02088466','n02088632','n02089078','n02089867'\n",
    ",'n02089973','n02090379','n02090622','n02090721'\n",
    ",'n02091032','n02091134','n02091244','n02091467'\n",
    ",'n02091635','n02091831','n02092002','n02092339'\n",
    ",'n02093256','n02093428','n02093647','n02093754'\n",
    ",'n02093859','n02093991','n02094114','n02094258'\n",
    ",'n02094433','n02095314','n02095570','n02095889'\n",
    ",'n02096051','n02096177','n02096294','n02096437'\n",
    ",'n02096585','n02097047','n02097130','n02097209'\n",
    ",'n02097298','n02097474','n02097658','n02098105'\n",
    ",'n02098286','n02098413','n02099267','n02099429'\n",
    ",'n02099601','n02099712','n02099849','n02100236'\n",
    ",'n02100583','n02100735','n02100877','n02101006'\n",
    ",'n02101388','n02101556','n02102040','n02102177'\n",
    ",'n02102318','n02102480','n02102973','n02104029'\n",
    ",'n02104365','n02105056','n02105162','n02105251'\n",
    ",'n02105412','n02105505','n02105641','n02105855'\n",
    ",'n02106030','n02106166','n02106382','n02106550'\n",
    ",'n02106662','n02107142','n02107312','n02107574'\n",
    ",'n02107683','n02107908','n02108000','n02108089'\n",
    ",'n02108422','n02108551','n02108915','n02109047'\n",
    ",'n02109525','n02109961','n02110063','n02110185'\n",
    ",'n02110341','n02110627','n02110806','n02110958'\n",
    ",'n02111129','n02111277','n02111500','n02111889'\n",
    ",'n02112018','n02112137','n02112350','n02112706'\n",
    ",'n02113023','n02113186','n02113624','n02113712'\n",
    ",'n02113799','n02113978','n02123045','n02123159'\n",
    ",'n02123394','n02123597','n02124075','n02125311'\n",
    ",'n02127052']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查找，显示，并删除异常数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "import numpy as np\n",
    "def find_dirty_img(train_dir, dogs_cats,top = 10):\n",
    "    dirty_imgs = []\n",
    "    pathlist = os.listdir(train_dir)\n",
    "    model = ResNet50(weights='imagenet')\n",
    "    for i in pathlist:\n",
    "        img = image.load_img(train_dir + i, target_size=(224, 224))  \n",
    "        img = image.img_to_array(img)  \n",
    "        x = np.expand_dims(img, axis=0)  \n",
    "        x = preprocess_input(x)  \n",
    "        preds = model.predict(x)\n",
    "        preds = decode_predictions(preds, top=top)[0] \n",
    "        preds_list = list(zip(*preds))[0]\n",
    "        if set(preds_list)&set(dogs_cats):\n",
    "            continue\n",
    "        else:\n",
    "            dirty_imgs.append(train_dir + i)\n",
    "            imgs = cv2.imread(train_dir + i)\n",
    "            plt.imshow(imgs)\n",
    "            plt.axis('on')\n",
    "            plt.title(i)\n",
    "            plt.show()\n",
    "    return dirty_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirty_imgs = find_dirty_img(train_dir, dogs_cats, top = 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dirty_imgs, len(dirty_imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到共筛选出63个异常值，我们现在将他们从文件中删除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for img in dirty_imgs:\n",
    "    os.remove(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "删除后的文件总数有24935个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list = os.listdir(train_dir)\n",
    "len(train_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在剔除脏文件后，我们对train的文件进行链接，用来做后面的算法实施。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# symbol link train2\n",
    "train_filenames = os.listdir(train_dir)\n",
    "train_cat = list(filter(lambda x:x[:3] == 'cat', train_filenames))\n",
    "train_dog = list(filter(lambda x:x[:3] == 'dog', train_filenames))\n",
    "\n",
    "for i in tqdm(range(len(train_cat))):\n",
    "    os.symlink(train_dir + train_cat[i], img_train2 + \"cat/\" + train_cat[i])\n",
    "for i in tqdm(range(len(train_dog))):\n",
    "    os.symlink(train_dir + train_dog[i], img_train2 + \"dog/\" + train_dog[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#link valid train\n",
    "for i in tqdm(range(len(train_cat))):\n",
    "    if i < (len(train_cat) * 0.2):\n",
    "        os.symlink(train_dir + train_cat[i], img_valid_cat + train_cat[i])\n",
    "    else:\n",
    "        os.symlink(train_dir + train_cat[i], img_train_cat + train_cat[i])\n",
    "for i in tqdm(range(len(train_dog))):\n",
    "    if i < (len(train_cat) * 0.2):\n",
    "        os.symlink(train_dir + train_dog[i], img_valid_dog+ train_dog[i])\n",
    "    else:\n",
    "        os.symlink(train_dir + train_dog[i], img_train_dog + train_dog[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、算法实施\n",
    "我首先实现一个类，可以进行对一些基础模型进行迁移，和fine-tune的操作。为后面的模型训练提取等做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.image import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "class CNN(object):\n",
    "    np.random.seed(23)\n",
    "\n",
    "    def __init__(self, model, train_dir, val_dir, test_dir, train_full_dir,\n",
    "                 epochs, patience, batch_size=32,\n",
    "                 img_sz=(224, 224), preprocess_func=None):\n",
    "        self.train_dir = train_dir\n",
    "        self.train_full_dir = train_full_dir\n",
    "        self.val_dir = val_dir\n",
    "        self.test_dir = test_dir\n",
    "        self.epochs = epochs\n",
    "        self.patience = patience\n",
    "        self.img_sz = img_sz\n",
    "        if preprocess_func:\n",
    "            self.preprocess_input = preprocess_func\n",
    "\n",
    "        train_generator = ImageDataGenerator(\n",
    "            preprocessing_function=self.preprocess_input,\n",
    "            rotation_range=10,\n",
    "            width_shift_range=0.05,\n",
    "            height_shift_range=0.05,\n",
    "            shear_range=0.1,\n",
    "            zoom_range=0.1,\n",
    "            horizontal_flip=True)\n",
    "        self.train_ge = train_generator.flow_from_directory(\n",
    "            self.train_dir,\n",
    "            target_size=self.img_sz,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            class_mode='binary')\n",
    "\n",
    "        val_generator = ImageDataGenerator()\n",
    "        self.val_ge = val_generator.flow_from_directory(\n",
    "            self.val_dir,\n",
    "            target_size=self.img_sz,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            class_mode='binary')\n",
    "\n",
    "        self.test_ge = val_generator.flow_from_directory(\n",
    "            self.test_dir,\n",
    "            target_size=self.img_sz,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            class_mode=None)\n",
    "\n",
    "        input_tensor = Input((img_sz[0], img_sz[1], 3))\n",
    "        self.base_model = model(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
    "        self.model_name = self.base_model.name\n",
    "        for layer in self.base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        x = self.base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        output = Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "\n",
    "        self.model = Model(inputs=self.base_model.input, outputs=output)\n",
    "        self.model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def fit(self):\n",
    "        check_point = ModelCheckpoint(\n",
    "            self.model_name + '-freeze.hdf5',\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True)\n",
    "\n",
    "        self.model.fit_generator(\n",
    "            self.train_ge,\n",
    "            steps_per_epoch=650,\n",
    "            epochs=self.epochs,\n",
    "            validation_data=self.val_ge,\n",
    "            validation_steps=150,\n",
    "            callbacks=[check_point])\n",
    "\n",
    "    def fit_finetune(self, fine_tune_layer, epochs = self.epochs):\n",
    "        for layer in self.model.layers:\n",
    "            layer.trainable = False\n",
    "        for layer in self.model.layers[-fine_tune_layer:]:\n",
    "            layer.trainable = True\n",
    "        self.model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        check_point = ModelCheckpoint(\n",
    "            self.model_name + 'finetune-freeze.hdf5',\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True)\n",
    "\n",
    "        self.model.fit_generator(\n",
    "            self.train_ge,\n",
    "            steps_per_epoch=1250,\n",
    "            epochs=epochs,\n",
    "            validation_data=self.val_ge,\n",
    "            validation_steps=150,\n",
    "            callbacks=[check_point])\n",
    "\n",
    "    def summary(self, bestmode = False):\n",
    "        self.model.summary()\n",
    "\n",
    "    def predict(self, name):\n",
    "        self.predict_ge = self.best_model.predict_generator(self.test_ge, verbose=1)\n",
    "        self.predict_ge = self.predict_ge.clip(min=0.005, max=0.995)\n",
    "\n",
    "        df = pd.read_csv(\"sample_submission.csv\")\n",
    "        for i, fname in enumerate(self.test_ge.filenames):\n",
    "            index = int(fname.split('/')[1].split('.')[0])\n",
    "            df.set_value(index - 1, 'label', self.predict_ge[i])\n",
    "\n",
    "        outfile = 'pred_' + name + '.csv'\n",
    "        print('Saving test result on: ' + outfile)\n",
    "        df.to_csv(outfile, index=None)\n",
    "        print(\"save_predict in: \" + outfile)\n",
    "        return df\n",
    "\n",
    "    def write_feature(self, name = self.model_name):\n",
    "        model = Model(self.model.input, self.model.layers[-3].output)\n",
    "        print('The output of model: ', model.output)\n",
    "\n",
    "        print('Data augmentation')\n",
    "        gen = ImageDataGenerator(\n",
    "            preprocessing_function=self.preprocess_input,\n",
    "            rotation_range = 10,\n",
    "            zoom_range = 0.1,\n",
    "            width_shift_range = 0.05,\n",
    "            height_shift_range = 0.05,\n",
    "            channel_shift_range=10,\n",
    "            shear_range=5,\n",
    "            horizontal_flip=True,\n",
    "        )\n",
    "\n",
    "        train_gen = gen.flow_from_directory(\n",
    "            self.train_full_dir,\n",
    "            target_size=self.img_sz,\n",
    "            shuffle=False,\n",
    "            batch_size=128,\n",
    "            class_mode=None,\n",
    "        )\n",
    "\n",
    "        self.train_gen_full = train_gen\n",
    "        test_gen = self.test_ge\n",
    "\n",
    "        print('feature from train data ...' )\n",
    "        train = model.predict_generator(train_gen, verbose=1)\n",
    "        print('feature from test data ...' )\n",
    "        test = model.predict_generator(test_gen, verbose=1)\n",
    "\n",
    "        fn = \"feature_%s.h5\"%name\n",
    "        print('Write feature to file: ' + fn)\n",
    "        with h5py.File(fn) as h:\n",
    "            h.create_dataset(\"train\", data=train)\n",
    "            h.create_dataset(\"label\", data=train_gen.classes)\n",
    "            h.create_dataset(\"test\", data=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单模型fine-tune 并提取特征向量\n",
    "### Xception\n",
    "全冻结模型进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于xception进行finetune，finetune最后28层。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dog-project]",
   "language": "python",
   "name": "conda-env-dog-project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
